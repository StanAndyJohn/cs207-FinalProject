{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS207 Milestone2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The premise of this project relies on the question - \"Why are derivatives important?\". Derivatives are used to explain the instantaneous rate of change and is a building block in the mathematics behind optimization problems. They are a fundamental tool of calculus. Since real world problems are continuous and seldomly neatly segmented into discrete blocks, we need derivatives to help us understand the patterns that we see occurring in the real world. This can range from applications in the field of medicine to that of rocket science. The process of finding a derivative is called differentiation and is a fundamental operation in the field of calculus.\n",
    "\n",
    "While we learn differentiation by hand, more complex equations require us to use computers to compute the results.Automatic Differentiation is a way for computers to break down the steps required to compute the derivative of a function. Computers no matter their level of processing power, all have a sequence of elementary arithmetic operations and elementary functions that can be put together via the chain rule to calculate complex, higher-order tasks. Our software library will take in these complex, higher-order tasks and produce accurate results via Automatic Differentiation. \n",
    "\n",
    "Another method that computers use to compute derivatives is Symbolic Differentiation. Symbolic Differentiation requires complete knowledge of all control flow in the program and can get exponentially large which results in a slow-down of the system when computing complex workloads. We choose to focus on the Automatic Differentiation implementation in this project because of this reason.\n",
    "\n",
    "We are building a software library to allow users to utilize Automatic Differentiation methods on machine calculations in order to guarantee the precision and accuracy of their calculations (this is particularly useful in scientific computing scenarios). \n",
    "\n",
    "\n",
    "## Background\n",
    "#### What is AD?\n",
    "As mentioned in the introduction, computers are able to compute elementary arithmetic operations and functions extremely well. When a computer is tasked with a derivative equation however, they can end up utilizing step sizes for limit operation that are too big or too small. When this occurs, the delta between the approximation and the actual value can vary significantly and randomly. In most computing scenarios, and especially scientific ones, accuracy is required and non-negotiable. Therefore, we turn to automatic differentiation to save the day. AD utilizes the simple chain rule - that the derivative of each sub-expression can be calculated recursively to obtain the final derivatives - to overcome the problem of inaccuracy that computers are presented with.\n",
    "\n",
    "#### Why AD?\n",
    "\n",
    "There are three kinds of differentiation: Numerical Differentiation, Symbolic Differentiation, and Automatic Differentiation. \n",
    "\n",
    "_Numerical Differentiation_ works by estimating the derivative of a mathematical function using values of the function and is done by completing finite difference approximations. This way of calculating a derivative requires a lot of computation when faced with a high-dimensional function and therefore cannot scale into more complex endeavors.\n",
    "\n",
    "*Symbolic Differentiation* utilizes the product rule and the chain rule to restructure the original function into a simplified expression that can be handled by the computer for more efficient calculations. Symbolic Differentiation requires access to and transformation of source code. It can be expensive to build and performs redundant computation.\n",
    "\n",
    "*Automatic Differentiation* also uses the chain rule but does so by breaking components of the function down into elementary functions that can be computed with high accuracy and speed allowing for better utilization of computing resources in application. This method is simple to implement and verify. It is also easy to use while producing accurate results at the level that Symbolic Differentiation produces.\n",
    "\n",
    "#### Derivatives\n",
    "The derivative of a function measures the sensitivity of the function output with respect to its input value. The derivative is often described as the 'instantaneous rate of change'.\n",
    "\n",
    "\n",
    "$$f'(a)=\\lim _{h\\to 0}{\\frac {f(a+h)-f(a)}{h}}$$\n",
    "\n",
    "#### Chain Rule\n",
    "\n",
    "The chain rule allows us to break down the computation of a derivative of an object comprised of two or more functions as can be seen in the example below. It is a fundamental building block towards understanding how Automatic Differentation works.\n",
    "\n",
    "$${\\frac  {dz}{dx}}={\\frac  {dz}{dy}}\\cdot {\\frac  {dy}{dx}}$$\n",
    "\n",
    "#### How does AD work?\n",
    "An example of how AD works is provided below using the chain rule:\n",
    "\n",
    "\\begin{aligned}\n",
    "y=f(g(h(x)))=f(g(H(w_{0})))=f(g(w_{1}))=f(w_{2})=w_{3}\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "w_{0}=x\\\\\n",
    "w_{1}=h(w_{0})\\\\\n",
    "w_{2}=g(w_{1})\\\\\n",
    "w_{3}=f(w_{2})=y\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{y}}{\\partial{x}} = \\frac{\\partial{y}}{\\partial{w_{2}}} \\cdot \\frac{\\partial{w_{2}}}{\\partial{w_{1}}} \\cdot \\frac{\\partial{w_{1}}}{ \\partial{x}}\n",
    "$$\n",
    "\n",
    "Forward mode states that goes from the inside to the outside, while reverse mode is from the outside to the inside. In the case above:\n",
    "\n",
    "Forward mode calculates: $\\frac{\\partial{w_{i}}}{\\partial{x}} = \\frac{\\partial{w_{i}}}{\\partial{w_{i-1}}}\\cdot \\frac{\\partial{w_{i-1}}}{\\partial{x}}$ and $w_3 = y$\n",
    "\n",
    "Reverse mode calculates: $\\frac{\\partial{y}}{\\partial{w_{i}}} = \\frac{\\partial{y}}{\\partial{w_{i+1}}} \\cdot \\frac{\\partial{w_{i+1}}}{\\partial{w_{i}}}$ and $w_{0} = x$\n",
    "\n",
    "\n",
    "#### Forward and Reverse Mode\n",
    "\n",
    "_Forward Mode_\n",
    "\n",
    "Forward mode applies the chain rule to each basic operation in a forward trace - at every stage it will evaluate the operater as well as the gradient in lockstep. \n",
    "\n",
    "_Reverse Mode_\n",
    "\n",
    "There are two phases in reverse mode: the forward phase and the backward phase. During the forward phase, all intermediate variables are evaluated and the results are stored in memory. In the backward phase, the chain rule is utilized to propagate back the derivatives.\n",
    "\n",
    "#### Elementary Functions\n",
    "\n",
    "Elementary functions are defined as funtioncs of one variable which is a finite sum, product, and/or composition of rational functions, sin, cos, exp, and their inverse functions. We've listed a sample of elementary functions and their corresponding derivatives in the table below.\n",
    "\n",
    "\n",
    "|$$f(x)$$ |\t$$f'(x)$$\n",
    "| :- |:-|\n",
    "|$$c$$|\t$0$|\n",
    "|$$x$$ |$1$|\n",
    "|$$x^n$$|$$nx^{n-1}$$|\n",
    "|$$\\frac{1}{x}$$|$$-\\frac{1}{x^2}$$|\n",
    "|$$e^x$$|$$e^x$$|\n",
    "|$$log_ax$$|$$\\frac{1}{x \\ln a}$$|\n",
    "|$$\\ln x$$|$$\\frac{1}{x}$$|\n",
    "|$$\\sin(x)$$|$$\\cos(x)$$|\n",
    "|$$\\cos(x)$$|$$-\\sin(x)$$|\n",
    "|$$\\tan(x)$$|$$\\frac{1}{\\cos^2x}$$|\n",
    "\n",
    "#### Jacobians \n",
    "The Jacobian matrix of a vector-valued function is the matrix of all of its first-order partial derivatives as can be seen in the matrix representation below. If the Jacobian matrix is a square matrix, it is known as the Jacobian determinant.\n",
    "<img src=\"./pictures/Figure4.png\">\n",
    "\n",
    "#### Computational Graph Representation\n",
    "\n",
    "The computational graph representation allows us to visualize how the computer is breaking down the calculations into the fundamental steps we described above. For example, in lecture, we took the following equation and broke it down into the respective steps for calculation. The graph can be viewed as a flowchart and is easier for the viewer to understand how the elementary functions operate on each variable in the process.\n",
    "\n",
    "Example Equation:\n",
    "$$f(x) = 6\\exp(x^2)+\\cos(2x)$$\n",
    "\n",
    "\n",
    "Computational Graph:\n",
    "<img src=\"./pictures/Figure3.png\">\n",
    "\n",
    "\n",
    "Evaluation Table:\n",
    "\n",
    "| Trace | Elementary Function | Current Value | Elementary Function Derivative | Delta x       \n",
    "| :- |:-| :- | :- | :-\n",
    "|$x_{1}$| $x$ | $x$ | 1| 1 <br>\n",
    "|$x_{2}$ | $x_{1}^{2}$ | $x_{1}^{2}$ | $2x_{1}$ | $2x_{1}$  <br>\n",
    "|$x_{3}$ | $exp(x_{2}^{2})$|$exp(x_{2}^{2})$ |$2xexp(x_{2}^{2})$| $2xexp(x_{2}^{2})$ <br>\n",
    "|$x_{4}$|$6x_{3}$| $6x_{3}$ |$x_{3}$|$x_{3}$ <br>\n",
    "|$x_{5}$| $2x_{1}$ | $2x_{1}$ |$x_{1}$ | $x_{1}$ <br>\n",
    "|$x_{6}$|$cos(x_{5})$ |$cos(x_{5})$|$sin(x_{5})$|$sin(x_{5})$ <br>\n",
    "|$x_{7}$|$x_{4}+x_{6}$|$x_{4}+x_{6}$|$2xexp(x_{3}^{2})+sin(x_{5})$ | $2xexp(x_{3}^{2})+sin(x_{5})$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How to Use PackageName\n",
    "### How to Use *AutoDiff*\n",
    "\n",
    "\n",
    "#### Installation via Github (for developers and users)\n",
    "Users are able to install our package via Github through following commands:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/StanAndyJohn/cs207-FinalProject.git\n",
    "\n",
    "```\n",
    "Create a virtual environment and call it `env`.\n",
    "```bash\n",
    "virtualenv env\n",
    "```\n",
    "\n",
    "Activate the virtual environment and install the dependencies.\n",
    "```bash\n",
    "source env/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Open a Python interpreter on the virtual environment and import the module\n",
    "\n",
    "```python\n",
    ">>> import autoDiff.autoDiff as ad\n",
    "```\n",
    "\n",
    "#### Introduction to basic usage of the package\n",
    "\n",
    "After successful installation, the user will first import our package.\n",
    "```python\n",
    ">>> import autoDiff.autoDiff as ad\n",
    "```\n",
    "We have the following options provided:\n",
    "\n",
    "##### Scalar functions of scalar values\n",
    "Goal:  gradient of the expression $f(x) = alpha * x + 6$.\n",
    "Input:  a variable x and then the symbolic expression for `f`.\n",
    "```python\n",
    ">>> x = ad.Variable(7, name='x')\n",
    ">>> f = 7 * x + 6\n",
    "```\n",
    "Special function: sin,cos,exp,etc.\n",
    "```python\n",
    ">>> f = 7 * ad.func.sin(x) + 6\n",
    "```\n",
    "Goal: evaluate the gradients of f with respect to x.\n",
    "```python\n",
    ">>> print(f.val, f.der)\n",
    "```\n",
    "f.val returns value of f \n",
    "f.der returns gradient of f with respect to x.\n",
    "\n",
    "Goal: second derivatives of f with respect to x\n",
    "```python\n",
    ">>> print(f.der2)\n",
    "```\n",
    "f.der2 returns second derivative of f with respect to x.\n",
    "\n",
    "##### Scalar functions of vectors - Type 1\n",
    "Goal: gradient of the expression $f(x_1,x_2) = x_1 x_2 + x_1$. \n",
    "Input: two variables `x1` and `x2` and the symbolic expression for `f`.\n",
    "```python\n",
    ">>> x1 = ad.Variable(2,name='x1')\n",
    ">>> x2 = ad.Variable(3,name='x2')\n",
    ">>> f = x1 * x2 + x1\n",
    "```\n",
    "Goal: values and gradients of f with respect to x1 and x2\n",
    "```python\n",
    ">>> print(f.val, f.der)\n",
    "```\n",
    "f.val returns dictionaries of values of f \n",
    "f.der returns dictionaries of gradients of f with respect to x1 and x2.\n",
    "\n",
    "Goal: second derivatives of f with respect to x1 and x2\n",
    "```python\n",
    ">>> print(f.der2)\n",
    "```\n",
    "\n",
    "f.der2 will then contain dictionaries of values and gradients of f with respect to x1 and x2, i.e., $\\frac{\\partial^2 f}{\\partial x_1^2}$, $\\frac{\\partial^2 f}{\\partial x_2^2}$, $\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}$ and $\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}$ as a dictionary with keys `'x1x1'`, `'x2x2'`, `'x1x2'` and `'x2x1'` respectively.\n",
    "\n",
    "##### Scalar functions of vectors - Type 2\n",
    "Goal: gradient of the expression $f(x_1, x_2) = (x_1 - x_2)^2$ where $x_1$ and $x_2$ are vectors themselves. \n",
    "\n",
    "Input  two variables `x1` and `x2` and the symbolic expression for `f`.\n",
    "```python\n",
    ">>> x1 = ad.Variable([2, 3, 4], name='x1')\n",
    ">>> x2 = ad.Variable([3, 2, 1], name='x2')\n",
    ">>> f = (x1 - x2)**2\n",
    "```\n",
    "Goal: values and gradients of f with respect to $x_1$ and $x_2$\n",
    "```python\n",
    ">>> print(f.val, f.der, f.der2)\n",
    "```\n",
    "\n",
    "##### Vector functions of vectors\n",
    "Goal: gradients of the system of functions \n",
    "$$f_1 = x_1 x_2 + x_1$$\n",
    "$$f_2 = \\frac{x_1}{x_2}$$\n",
    "\n",
    "i.e.\n",
    "$$\\mathbf{f}(x1,x2)=(f_1(x_1,x_2),f_2(x_1,x_2))$$\n",
    "Input: two variables `x1` and `x2` and the symbolic expression for `f`.\n",
    "```python\n",
    ">>> x1 = ad.Variable(3, name = 'x1')\n",
    ">>> x2 = ad.Variable(2, name = 'x2')\n",
    ">>> f1 = x1 * x2 + x1\n",
    ">>> f2 = x1 / x2\n",
    "```\n",
    "Goal:  the gradients of f with respect to x1 and x2\n",
    "```python\n",
    ">>> print(f1.val, f2.val, f1.der, f2.der)\n",
    "```\n",
    "The Jacobian $\\mathbf{J}(\\mathbf{f})$ =(f1', f2') = (f1.der, f2.der)\n",
    "\n",
    "Goal: second derivatives (Hessian matrix)\n",
    "```python\n",
    ">>> print(f1.der2, f2.der2)\n",
    "```\n",
    "\n",
    "## Software Organization \n",
    "###### Discuss how you plan on organizing your software package.\n",
    "\n",
    "- What will the directory structure look like?\n",
    "```bash\n",
    "├── AutoDiff\n",
    "│   ├── __init__.py\n",
    "│   ├── AutoDiff.py\n",
    "│   └── file2.py\n",
    "├── demos\n",
    "│   ├── demo1.py\n",
    "│   └── demo2.py\n",
    "├── tests\n",
    "│   ├── testforBasicFeature.py\n",
    "│   └── testforAdditionalFeatures.py\n",
    "├── docs\n",
    "│   ├── milestone1.ipynb\n",
    "│   └── milestone2.ipynb\n",
    "├── .codecov.yml\n",
    "├── .travis.yml\n",
    "├── LICENSE.md\n",
    "├── README.md\n",
    "└── requirements.txt\n",
    "```\n",
    "- What modules do you plan on including? What is their basic functionality?\n",
    "\n",
    "   - \\_\\_init__.py: initializes the package\n",
    "   - AutoDiff.py: implements basic data structure and algorithms of the forward mode of automatic differentiation and operator overloading methods. element_func.py is imported in AutoDiff.py so that the elementary functions can be called within this parent class.\n",
    "   - element_func.py: implements the elementary functions that complete the calculations for forward mode operation.\n",
    "\n",
    "\n",
    "- Where will your test suite live? Will you use TravisCI? CodeCov?\n",
    "\n",
    "   Our test suite will be under the tests folder. We plan to have 2 test files, one for AD, the other for additional features. TravisCI and CodeCov will be used in our project. \n",
    "   \n",
    "   We have put all of our test files in the directory under cs207-FinalProject/test/ and have set up Travis Cl to validate each pull request for build completion. Codecov has also been setup to validate the total numebr of lines that have been tested.\n",
    "    \n",
    "    \n",
    "- How will you distribute your package (e.g. PyPI)?\n",
    "\n",
    "   We will distribute our package on PyPI. More information regarding how to use our package is discussed in the How To Use section. \n",
    "   \n",
    "   \n",
    "- How will you package your software? Will you use a framework? If so, which one and why? If not, why not?\n",
    "   \n",
    "   We will closely follow the instructions on https://packaging.python.org/tutorials/packaging-projects/ to package our software. As of now, we decide not to use a framework to package our software because our software will include only some Python modules and other files which do not depend on other frameworks. A standard Python’s native packaging should be sufficient for our software.\n",
    "   \n",
    "   \n",
    "- Other considerations?\n",
    "\n",
    "  If time allows, we are thinking of building a user friendly UI for our software. Some web frameworks for Python are Django or Flask.\n",
    "\n",
    "\n",
    "\n",
    "## Implementation\n",
    "###### Discuss how you plan on implementing the forward mode of automatic differentiation.\n",
    "\n",
    "- What are the core data structures?\n",
    "   - dictionary: use to keep track of the partial derivatives\n",
    "   \n",
    "   \n",
    "- What classes will you implement? What method and name attributes will your classes have?\n",
    "\n",
    "##### Elementary Functions\n",
    "\n",
    "We've included the elementary funtions listed below in elementy_func.py: <br>\n",
    "\n",
    "- sin(x)<br> \n",
    "- sinh(x)<br> \n",
    "- arcsin(x)<br> \n",
    "- cos(x)<br> \n",
    "- cosh(x)<br> \n",
    "- arccos(x)<br> \n",
    "- tan(x)<br> \n",
    "- tanh(x)<br> \n",
    "- arctan(x)<br> \n",
    "- exp(x)<br> \n",
    "- log(x)<br>\n",
    "- sqrt(x)<br>\n",
    "\n",
    "These methods take an input variable object or scalar and return a new variable class object or scalar after the operation has been completed using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classes | Description | Attributes | Methods         \n",
    "| :- |:------------- | :- | :-\n",
    "|Variable|  an auto-differentiation class with the overloaded operators | der: dictionary of derivatives | Operator Overloading Methods: \\_\\_add__, \\_\\_radd__, \\_\\_sub__, \\_\\_rsub__, \\_\\_mul__, \\_\\_rmul__, \\_\\_pow__, \\_\\_rpow__, \\_\\_itruediv__, \\_\\_rtruediv__, \\_\\_pos__, \\_\\_neg__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What external dependencies will you rely on?\n",
    "   \n",
    "   - numpy: ~1.17.x\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How will you deal with elementary functions like sin, sqrt, log, and exp (and all the others)?\n",
    "  \n",
    "  We will implement these elementary functions in element_func.py which will be imported in our Variable class in autoDiff.py. <br>\n",
    "  \n",
    "- What aspects have you not implemented yet? What else do you plan on implementing?\n",
    " \n",
    "  We have not implemented the Jacobian matrix in milestone2 but will include that in our final deliverable.\n",
    "\n",
    "### Optimization Extensions\n",
    "#### Root Finding\n",
    "\n",
    "\n",
    "*Background*: Newton’s Method is a root-finding algorithm that produces successively better approximations to the roots of a function. The idea is to start with an initial guess which is reasonably close to the true root, then to approximate the function by its tangent line using calculus, and finally to compute the x-intercept of this tangent line by elementary algebra. At each step t of the process, Newton’s Method finds a line that is tangent to the function at point x(t). Where the tangent line crosses the x-axis serves as the new guess for x(t+1). Newton’s Method can be used to find a minimum or maximum of a function and is also a very efficient way to find the multiplicative inverses of numbers and power series.\n",
    "\n",
    "*Implementation*:\n",
    "\n",
    "- *NewtonRoot*: this method returns a root of the function\n",
    "    - Input: \n",
    "        - *f*: function of multiple scalars with the arguments passed in as a list.\n",
    "        - *x*: list of ad.var objects and is the initial guess for a root of the function f.\n",
    "        - *iters*: the maximum number of iterations to run the algorithm.\n",
    "        - *tol*: the tolerance specified for running the algorithm. If the size of the update step is smaller than this tolerance, then the algorithm  will add that step as the final operation prior to terminating the calculation.\n",
    "        - *der_shift*: stores a random start value for the derivative in the case that a root guess has a derivative of 0.\n",
    "    - Return: \n",
    "        - *root*: the val attribute contains the root which was found in min(iters, t) in a numpy array format.\n",
    "        - *input_list*: holds the number of steps of the algorithm and the dimension of the root in numpy array format.\n",
    "        - *f_list*: holds the consecutive number of steps of the function f when using var_path as the input.\n",
    "        \n",
    "#### Gradient Descent\n",
    "\n",
    "*Background*: Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. A cost function is defined by the user after which the gradient descent algorithm is used to minimize the cost function. By minimizing the cost function, the system increases the accuracy of the prediction model.\n",
    "\n",
    "*Implementation*:\n",
    "\n",
    "- *GradientDescent*: this method finds the local minimum of a function f.\n",
    "\n",
    "    - Input:\n",
    "        - *f*: the cost function that will be minimized.\n",
    "        - *x*: list of ad.var objects and represents the initial guess for a root of the cost function f.\n",
    "        - *iters*: maximum number of iterations to the run the algorithm\n",
    "        - *tol*: the tolerance specified for running the algorithm. \n",
    "        - *eta*: defines the learning rate for the gradient descent algorithm.\n",
    "        - *data*: represents the dataset that the gradient descent will be run on. It uses a numpy array to store its contents.\n",
    "\n",
    "    - Return: \n",
    "        - *minimum*: contains the optimal solution and the derivative at that point.\n",
    "        - *input_list*: contains the path of the input variables.\n",
    "        - *f_list*: stores the path of the objective function.\n",
    "\n",
    "#### BFGS\n",
    "\n",
    "*Background*: The Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is an iterative method for solving unconstrained nonlinear optimization problems. The BFGS method approximates Newton's method, using both first and second derivatives of the function. Quasi-Newton methods are generalizations of the secant method to find the root of the first derivative for multidimensional problems.\n",
    "\n",
    "*Implementation*:\n",
    "\n",
    "- *BFGS*: this method that runs this optimization algorithm.\n",
    "    - Input:\n",
    "        - *f*:  defines the function to be optimized.\n",
    "        - *x*: specifies the initial guess for the minimum\n",
    "        - *iters*: specifies the max number of iterations\n",
    "        - *tol*: specifies the tolerance for the algorithm. The calculations will be stopped if the next step size is less than tol.\n",
    "    - Return:\n",
    "        - *minimum*: holds the min root / optimal solution that the algorithm derives.\n",
    "        - *input_list*: stores the path of the input variables\n",
    "        - *f_list*: stores the path of the objective function\n",
    "\n",
    "#### Quadratic Splines\n",
    "\n",
    "*Background*: A spline function consists of polynomial pieces on subintervals joined together with certain continuity conditions. Quadratic splines are 2nd degree splines. A spline function of degree k having knots x0, x1, · · · , xn is a function S such that: \n",
    "- on each interval [xi−1, xi ], S is a polynomial of degree ≤ k \n",
    "- S has a continuous (k − 1)st derivative on [x0, xn].\n",
    "\n",
    "*Implementation*: \n",
    "\n",
    "- *quad_spline_coeff*: this method constructs the matrix for quadratic spline calculation and returns the coefficients of quadratic functions.\n",
    "    - Input:\n",
    "        - *f*: defines the function to be optimized.\n",
    "        - *xMin*: specifies the left endpoint of the interval\n",
    "        - *xMax*: specifies the right endpoint of the interval\n",
    "        - *nint*: specifies the number of intervals for slicing the desired function.\n",
    "\n",
    "    - Return:\n",
    "        - *y*: stores the output of the optimization\n",
    "        - *a*: stores the square matrix of Ax = y\n",
    "        - *coeffs*: stores the coefficients a_i, b_i, c_i\n",
    "        - *ks*: stores the points of interest\n",
    "\n",
    "\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Additional work that can be done for this software library include adding the Generalized Minimal Residual Method (GMRES) for optimization as well as building customized solution for applications in industry use cases such as:\n",
    "- Computational Fluid Dynamics\n",
    "- Chemical Engineering\n",
    "- Weather and Climate Modeling\n",
    "- Structural Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
