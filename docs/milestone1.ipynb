{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS207 Milestone1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "We are building a software library to allow users to utilize Automatic Differentiation methods on machine calculations in order to guarantee the precision and accuracy of their calculations (this is particularly useful in scientific computing scenarios). Automatic Differentiation is a way for computers to break down the steps required to compute the derivative of a function. \n",
    "\n",
    "Computers no matter their level of processing power, all have a sequence of elementary arithmetic operations and elementary functions that can be put together via the chain rule to calculate complex, higher-order tasks. Our software library will take in these complex, higher-order tasks and produce accurate results via Automatic Differentiation. \n",
    "\n",
    "## Background\n",
    "#### What is AD?\n",
    "As mentioned in the introduction, computers are able to compute elementary arithmetic operations and functions extremely well. When a computer is tasked with a derivative equation however, they can end up utilizing step sizes for limit operation that are too big or too small. When this occurs, the delta between the approximation and the actual value can vary significantly and randomly. In most computing scenarios, and especially scientific ones, accuracy is required and non-negotiable. Therefore, we turn to automatic differentiation to save the day. AD utilizes the simple chain rule - that the derivative of each sub-expression can be calculated recursively to obtain the final derivatives - to overcome the problem of inaccuracy that computers are presented with.\n",
    "\n",
    "\n",
    "#### How does AD work?\n",
    "An example of how AD works is provided below using the chain rule:\n",
    "\n",
    "\\begin{aligned}\n",
    "y=f(g(h(x)))=f(g(H(w_{0})))=f(g(w_{1}))=f(w_{2})=w_{3}\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "w_{0}=x\\\\\n",
    "w_{1}=h(w_{0})\\\\\n",
    "w_{2}=g(w_{1})\\\\\n",
    "w_{3}=f(w_{2})=y\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{y}}{\\partial{x}} = \\frac{\\partial{y}}{\\partial{w_{2}}} \\cdot \\frac{\\partial{w_{2}}}{\\partial{w_{1}}} \\cdot \\frac{\\partial{w_{1}}}{ \\partial{x}}\n",
    "$$\n",
    "\n",
    "Forward mode states that goes from the inside to the outside, while reverse mode is from the outside to the inside. In the case above:\n",
    "\n",
    "Forward mode calculates: $\\frac{\\partial{w_{i}}}{\\partial{x}} = \\frac{\\partial{w_{i}}}{\\partial{w_{i-1}}}\\cdot \\frac{\\partial{w_{i-1}}}{\\partial{x}}$ and $w_3 = 7$\n",
    "\n",
    "Reverse mode calculates: $\\frac{\\partial{y}}{\\partial{w_{i}}} = \\frac{\\partial{y}}{\\partial{w_{i+1}}} \\cdot \\frac{\\partial{w_{i+1}}}{\\partial{w_{i}}}$ and $w_{0} = x$\n",
    "\n",
    "\n",
    "## How to Use PackageName\n",
    "### How to Use *AutoDiff*\n",
    "\n",
    "There are two ways to install our package.\n",
    "\n",
    "#### Method 1: User installation via ```pip```\n",
    "Users are able to install our package via “pip” through following commands:\n",
    "\n",
    "Create a virtual environment and call it `env`.\n",
    "```bash\n",
    "virtualenv env\n",
    "```\n",
    "\n",
    "Activate the virtual environment and install the package.\n",
    "```bash\n",
    "source env/bin/activate\n",
    "pip install AutoDiff-StanAndyJohn\n",
    "```\n",
    "\n",
    "Open a Python interpreter on the virtual environment and import the module\n",
    "```python\n",
    ">>> import AutoDiff.AutoDiff as ad\n",
    "```\n",
    "\n",
    "#### Method 2: Installation via Github (for developers and users)\n",
    "Users are able to install our package via Github through following commands:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/StanAndyJohn/cs207-FinalProject.git\n",
    "\n",
    "```\n",
    "Create a virtual environment and call it `env`.\n",
    "```bash\n",
    "virtualenv env\n",
    "```\n",
    "\n",
    "Activate the virtual environment and install the dependencies.\n",
    "```bash\n",
    "source env/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Open a Python interpreter on the virtual environment and import the module\n",
    "\n",
    "```python\n",
    ">>> import AutoDiff.AutoDiff as ad\n",
    "```\n",
    "\n",
    "#### Introduction to basic usage of the package\n",
    "\n",
    "After successful installation, the user will first import our package.\n",
    "```python\n",
    ">>> import AutoDiff.AutoDiff as ad\n",
    "```\n",
    "We have the following options provided:\n",
    "\n",
    "##### Scalar functions of scalar values\n",
    "Goal:  gradient of the expression $f(x) = alpha * x + 6$.\n",
    "Input:  a variable x and then the symbolic expression for `f`.\n",
    "```python\n",
    ">>> x = ad.Variable(7, name='x')\n",
    ">>> f = 7 * x + 6\n",
    "```\n",
    "Special function: sin,cos,exp,etc.\n",
    "```python\n",
    ">>> f = 7 * ad.sin(x) + 6\n",
    "```\n",
    "Goal: evaluate the gradients of f with respect to x.\n",
    "```python\n",
    ">>> print(f.val, f.der)\n",
    "```\n",
    "f.val returns value of f \n",
    "f.der returns gradient of f with respect to x.\n",
    "\n",
    "Goal: second derivatives of f with respect to x\n",
    "```python\n",
    ">>> print(f.der2)\n",
    "```\n",
    "f.der2 returns second derivative of f with respect to x.\n",
    "\n",
    "##### Scalar functions of vectors - Type 1\n",
    "Goal: gradient of the expression $f(x_1,x_2) = x_1 x_2 + x_1$. \n",
    "Input: two variables `x1` and `x2` and the symbolic expression for `f`.\n",
    "```python\n",
    ">>> x1 = ad.Variable(2,name='x1')\n",
    ">>> x2 = ad.Variable(3,name='x2')\n",
    ">>> f = x1 * x2 + x1\n",
    "```\n",
    "Goal: values and gradients of f with respect to x1 and x2\n",
    "```python\n",
    ">>> print(f.val, f.der)\n",
    "```\n",
    "f.val returns dictionaries of values of f \n",
    "f.der returns dictionaries of gradients of f with respect to x1 and x2.\n",
    "\n",
    "Goal: second derivatives of f with respect to x1 and x2\n",
    "```python\n",
    ">>> print(f.der2)\n",
    "```\n",
    "\n",
    "f.der2 will then contain dictionaries of values and gradients of f with respect to x1 and x2, i.e., $\\frac{\\partial^2 f}{\\partial x_1^2}$, $\\frac{\\partial^2 f}{\\partial x_2^2}$, $\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}$ and $\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}$ as a dictionary with keys `'x1x1'`, `'x2x2'`, `'x1x2'` and `'x2x1'` respectively.\n",
    "\n",
    "##### Scalar functions of vectors - Type 2\n",
    "Goal: gradient of the expression $f(x_1, x_2) = (x_1 - x_2)^2$ where $x_1$ and $x_2$ are vectors themselves. \n",
    "\n",
    "Input  two variables `x1` and `x2` and the symbolic expression for `f`.\n",
    "```python\n",
    ">>> x1 = ad.Variable([2, 3, 4], name='x1')\n",
    ">>> x2 = ad.Variable([3, 2, 1], name='x2')\n",
    ">>> f = (x1 - x2)**2\n",
    "```\n",
    "Goal: values and gradients of f with respect to $x_1$ and $x_2$\n",
    "```python\n",
    ">>> print(f.val, f.der, f.der2)\n",
    "```\n",
    "\n",
    "##### Vector functions of vectors\n",
    "Goal: gradients of the system of functions \n",
    "$$f_1 = x_1 x_2 + x_1$$\n",
    "$$f_2 = \\frac{x_1}{x_2}$$\n",
    "\n",
    "i.e.\n",
    "$$\\mathbf{f}(x1,x2)=(f_1(x_1,x_2),f_2(x_1,x_2))$$\n",
    "Input: two variables `x1` and `x2` and the symbolic expression for `f`.\n",
    "```python\n",
    ">>> x1 = ad.Variable(3, name = 'x1')\n",
    ">>> x2 = ad.Variable(2, name = 'x2')\n",
    ">>> f1 = x1 * x2 + x1\n",
    ">>> f2 = x1 / x2\n",
    "```\n",
    "Goal:  the gradients of f with respect to x1 and x2\n",
    "```python\n",
    ">>> print(f1.val, f2.val, f1.der, f2.der)\n",
    "```\n",
    "The Jacobian $\\mathbf{J}(\\mathbf{f})$ =(f1', f2') = (f1.der, f2.der)\n",
    "\n",
    "Goal: second derivatives (Hessian matrix)\n",
    "```python\n",
    ">>> print(f1.der2, f2.der2)\n",
    "```\n",
    "\n",
    "## Software Organization \n",
    "###### Discuss how you plan on organizing your software package.\n",
    "\n",
    "- What will the directory structure look like?\n",
    "```bash\n",
    "├── AutoDiff\n",
    "│   ├── __init__.py\n",
    "│   ├── AutoDiff.py\n",
    "│   └── file2.py\n",
    "├── demos\n",
    "│   ├── demo1.py\n",
    "│   └── demo2.py\n",
    "├── tests\n",
    "│   ├── testforBasicFeature.py\n",
    "│   └── testforAdditionalFeatures.py\n",
    "├── docs\n",
    "│   ├── milestone1.ipynb\n",
    "│   └── milestone2.ipynb\n",
    "├── .codecov.yml\n",
    "├── .travis.yml\n",
    "├── LICENSE.md\n",
    "├── README.md\n",
    "└── requirements.txt\n",
    "```\n",
    "- What modules do you plan on including? What is their basic functionality?\n",
    "\n",
    "   - \\_\\_init__.py: initializes the package\n",
    "   - AutoDiff.py: implements basic data structure and algorithms of the forward mode of automatic differentiation, including elementary functions/methods and operator overloading methods\n",
    "\n",
    "\n",
    "- Where will your test suite live? Will you use TravisCI? CodeCov?\n",
    "\n",
    "   Our test suite will be under the tests folder. We plan to have 2 test files, one for AD, the other for additional features. TravisCI and CodeCov will be used in our project.\n",
    "    \n",
    "    \n",
    "- How will you distribute your package (e.g. PyPI)?\n",
    "\n",
    "   We will distribute our package on PyPI. More information regarding how to use our package is discussed in the How To Use section. \n",
    "   \n",
    "   \n",
    "- How will you package your software? Will you use a framework? If so, which one and why? If not, why not?\n",
    "   \n",
    "   We will closely follow the instructions on https://packaging.python.org/tutorials/packaging-projects/ to package our software. As of now, we decide not to use a framework to package our sofeware because our software will include only some Python modules and other files which do not depend on other frameworks. A standard Python’s native packaging should be sufficient for our software.\n",
    "   \n",
    "   \n",
    "- Other considerations?\n",
    "\n",
    "  If time allows, we are thinking of building a user friendly UI for our software. Some web frameworks for Python are Django or Flask.\n",
    "\n",
    "\n",
    "\n",
    "## Implementation\n",
    "###### Discuss how you plan on implementing the forward mode of automatic differentiation.\n",
    "\n",
    "- What are the core data structures?\n",
    "   - dictionary: use to keep track of the partial derivatives\n",
    "   \n",
    "   \n",
    "- What classes will you implement? What method and name attributes will your classes have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classes | Description | Attributes | Methods         \n",
    "| :- |:------------- | :- | :-\n",
    "|AutoDiff|  an auto-differentiation class with the overloaded operators | der: dictionary of derivatives | Elementary Functions/Methods: sin, sinh, arcsin, cos, cosh, arccos, tan, tanh, arctan, exp, log <br>Operator Overloading Methods: \\_\\_add__, \\_\\_radd__, \\_\\_sub__, \\_\\_rsub__, \\_\\_mul__, \\_\\_rmul__, \\_\\_pow__, \\_\\_rpow__, \\_\\_itruediv__, \\_\\_rtruediv__, \\_\\_pos__, \\_\\_neg__\n",
    "| AutoDiffTest | a class with the test methods for AutoDiff class | | Comprehensive test methods for each method in the AutoDiff class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What external dependencies will you rely on?\n",
    "   \n",
    "   - numpy: ~1.17.x\n",
    "   - scipy: ~1.3.x\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How will you deal with elementary functions like sin, sqrt, log, and exp (and all the others)?\n",
    "  \n",
    "  We will implement these elementary functions in our AutoDiff class in AutoDiff.py. Our AutoDiff class includes elementary functions mentioned in the class description above.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
